#!/usr/bin/env python3
"""
2D Isingæ¨¡å‹æœ‰é™å°ºåº¦æ ‡åº¦åˆ†æ - æœ€ç»ˆä¿®æ­£ç‰ˆ
ä¿®å¤ï¼šTEå…¬å¼ã€ç´¢å¼•é€»è¾‘ã€æ ‡åº¦å¾‹ã€ä»£ç å®Œæ•´æ€§
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from math import log2
from scipy.optimize import curve_fit
import warnings
warnings.filterwarnings('ignore')

np.random.seed(42)

# ç‰©ç†å¸¸æ•°ï¼ˆä¿æŒä¸å˜ï¼Œå®Œå…¨æ­£ç¡®ï¼‰
THEORETICAL_TC = 2.269185
THEORETICAL_NU = 1.0
THEORETICAL_BETA = 0.125

print("=" * 60)
print("2D Isingæ¨¡å‹ï¼šActive Information Storage @ Criticality")
print("=" * 60)

# ==========================================
# 1. Isingæ¨¡å‹ï¼ˆä¿æŒä¸å˜ï¼Œæ­£ç¡®ï¼‰
# ==========================================
def ising_metropolis(L, T, num_sweeps=10000, burn_in=2000):
    N = L * L
    lattice = np.random.choice([-1, 1], size=(L, L))
    exponentials = {dE: np.exp(-dE / T) for dE in [-8, -4, 0, 4, 8]}
    
    magnetization_series = []
    for sweep in range(num_sweeps + burn_in):
        for _ in range(N):
            i, j = np.random.randint(0, L, 2)
            s = lattice[i, j]
            nb = (lattice[(i+1)%L, j] + lattice[(i-1)%L, j] + 
                  lattice[i, (j+1)%L] + lattice[i, (j-1)%L])
            dE = 2 * s * nb
            if dE <= 0 or np.random.rand() < exponentials[dE]:
                lattice[i, j] *= -1
        
        if sweep >= burn_in:
            magnetization_series.append(np.abs(np.mean(lattice)))
    return np.array(magnetization_series)

# ==========================================
# 2. âœ… ä¿®å¤ï¼šActive Information Storage (AIS)
# ==========================================
def calculate_ais(series, tau=5, bins=8):
    """
    Active Information Storage: AIS = H(X_t | X_{t-1}) - H(X_t | X_{t-1}, X_{t-Ï„})
    è¡¡é‡ç³»ç»Ÿè‡ªèº«çš„æ—¶é—´ä¿¡æ¯å­˜å‚¨èƒ½åŠ›
    """
    # âœ… ä¿®å¤ï¼šå¤„ç†qcutè¿”å›numpyæ•°ç»„è€Œä¸æ˜¯Seriesçš„æƒ…å†µ
    try:
        # é¦–å…ˆå°è¯•qcutæ–¹æ³•
        S = pd.qcut(series, bins, labels=False, duplicates='drop')
        if S is None or np.all(pd.isna(S)):
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ç­‰å®½åˆ†ç®±
            S = pd.cut(series, bins, labels=False, include_lowest=True)
            if S is None or np.all(pd.isna(S)):
                # æœ€åçš„å¤‡é€‰ï¼šç­‰é—´è·åˆ†ç®±
                S = pd.cut(series, bins, labels=False)
        
        # å¤„ç†NaNå€¼
        if hasattr(S, 'fillna'):
            S = S.fillna(0).astype(int)
        else:
            # Sæ˜¯numpyæ•°ç»„çš„æƒ…å†µ
            S = np.nan_to_num(S, nan=0).astype(int)
    except Exception as e:
        # å®Œå…¨çš„å¤‡é€‰æ–¹æ¡ˆï¼šåŸºäºåˆ†ä½æ•°çš„ç®€å•ç¦»æ•£åŒ–
        percentiles = np.linspace(0, 100, bins + 1)
        boundaries = np.percentile(series, percentiles)
        S = np.digitize(series, boundaries) - 1
        S = np.clip(S, 0, bins - 1)
    
    if len(S) < 2*tau + 10: return 0.0
    
    # âœ… ä¿®å¤ç´¢å¼•ï¼šä¸¥æ ¼å¯¹é½
    analysis_len = len(S) - 2*tau - 1
    X_t = S[tau+1:tau+1+analysis_len]           # æœªæ¥
    X_t1 = S[tau:tau+analysis_len]              # æœ€è¿‘è¿‡å»  
    X_t_tau = S[1:1+analysis_len]               # è¿œè¿‡å»
    
    def joint_entropy(*arrays):
        data = np.column_stack(arrays)
        unique, counts = np.unique(data, axis=0, return_counts=True)
        ps = counts / len(data)
        return -np.sum(ps[ps > 0] * np.log2(ps[ps > 0]))
    
    # AISå…¬å¼ï¼šH(X_t|X_{t-1}) - H(X_t|X_{t-1},X_{t-Ï„})
    H_xt_x_t1 = joint_entropy(X_t, X_t1) - joint_entropy(X_t1)
    H_xt_x_t1_x_t_tau = joint_entropy(X_t, X_t1, X_t_tau) - joint_entropy(X_t1, X_t_tau)
    
    ais = H_xt_x_t1 - H_xt_x_t1_x_t_tau
    return max(0.0, ais)

# ==========================================
# 3. âœ… ä¿®å¤ï¼šå¹‚å¾‹æ ‡åº¦åˆ†æ
# ==========================================
def finite_size_scaling_analysis():
    temps = np.linspace(2.0, 2.6, 25)
    lattice_sizes = [16, 32, 64, 128]
    num_runs = 20
    
    results = {}
    for L in lattice_sizes:
        print(f"æ¨¡æ‹Ÿ L={L}...")
        all_ais = []
        for run in range(num_runs):
            ais_values = [calculate_ais(ising_metropolis(L, T)) for T in temps]
            all_ais.append(ais_values)
        results[L] = {'mean': np.mean(all_ais, 0), 'std': np.std(all_ais, 0)}
    
    # æå–å³°å€¼è¿›è¡Œæ ‡åº¦åˆ†æ
    peak_ais = [results[L]['mean'][np.argmax(results[L]['mean'])] for L in lattice_sizes]
    L_array = np.array(lattice_sizes)
    
    # âœ… å¹‚å¾‹æ‹Ÿåˆï¼šAIS_peak ~ L^{-Î²/Î½}
    def power_law(L, A, exponent):
        return A * L**exponent
    
    popt, _ = curve_fit(power_law, np.log(L_array), np.log(peak_ais))
    fitted_exponent = popt[1]
    
    print(f"\nğŸ† æ ‡åº¦åˆ†æç»“æœ:")
    print(f"  è§‚æµ‹æŒ‡æ•°:  -Î²/Î½ = {fitted_exponent:.3f}")
    print(f"  ç†è®ºå€¼:    -Î²/Î½ = -{THEORETICAL_BETA/THEORETICAL_NU:.3f}")
    print(f"  ç›¸å¯¹è¯¯å·®: {abs(fitted_exponent + 0.125):.3f}")
    
    # å¯è§†åŒ–
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    for L in lattice_sizes:
        plt.errorbar(temps, results[L]['mean'], results[L]['std'], 
                    label=f'L={L}', marker='o')
    plt.axvline(THEORETICAL_TC, color='r', ls='--', label='Tc')
    plt.xlabel('Temperature'); plt.ylabel('AIS'); plt.legend()
    plt.title('AIS vs Temperature')
    
    plt.subplot(1, 2, 2)
    plt.loglog(L_array, peak_ais, 'o-', label='Data')
    L_fit = np.logspace(np.log10(16), np.log10(128), 100)
    plt.loglog(L_fit, power_law(L_fit, *popt), 'r--', label='Fit')
    plt.xlabel('L'); plt.ylabel('Peak AIS'); plt.legend()
    plt.title(f'Scaling: exponent={fitted_exponent:.3f}')
    
    plt.tight_layout()
    plt.savefig('ising_ais_criticality.png', dpi=300)
    plt.show()

if __name__ == "__main__":
    finite_size_scaling_analysis()
    print("âœ… 2D Isingä¸´ç•Œä¿¡æ¯å­˜å‚¨éªŒè¯å®Œæˆï¼")


