#!/usr/bin/env python3
"""
2D Isingæ¨¡å‹æœ‰é™å°ºåº¦æ ‡åº¦åˆ†æ - å®Œå…¨ä¿®æ­£ç‰ˆ
ä¿®æ­£äº†æ‰€æœ‰è‡´å‘½é”™è¯¯ï¼šç‰©ç†å¸¸æ•°ã€æ¨¡æ‹Ÿæ­¥æ•°ã€ä¼ é€’ç†µè®¡ç®—
corrected_ising_final.py

ä¿®æ­£æ—¥æœŸï¼š2025-12-13
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from math import log2, sqrt
import os
import json
import glob
from scipy.optimize import curve_fit
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
np.random.seed(42)

# ==========================================
# 1. æ­£ç¡®çš„2D Isingæ¨¡å‹ç‰©ç†å¸¸æ•° (Onsager Exact Solution)
# ==========================================

# 2D Isingæ¨¡å‹çš„æ­£ç¡®ç‰©ç†å¸¸æ•° (Onsager Exact Solution)
THEORETICAL_TC = 2.269185  # æ­£ç¡®çš„ä¸´ç•Œæ¸©åº¦
THEORETICAL_NU = 1.0       # 2D Exact (Onsagerè§£)
THEORETICAL_BETA = 0.125   # 1/8 (Onsagerè§£)
THEORETICAL_GAMMA = 1.75   # 7/4 (Onsagerè§£) 
THEORETICAL_ALPHA = 0.0    # å¯¹æ•°å‘æ•£ (Onsagerè§£)

print("=" * 60)
print("2D Isingæ¨¡å‹ç‰©ç†å¸¸æ•° (Onsager Exact Solution)")
print("=" * 60)
print(f"ä¸´ç•Œæ¸©åº¦: Tc = {THEORETICAL_TC:.6f}")
print(f"å…³è”é•¿åº¦æŒ‡æ•°: Î½ = {THEORETICAL_NU:.1f}")
print(f"ä¸´ç•ŒæŒ‡æ•°: Î² = {THEORETICAL_BETA:.3f}")
print(f"ä¸´ç•ŒæŒ‡æ•°: Î³ = {THEORETICAL_GAMMA:.2f}")
print(f"ä¸´ç•ŒæŒ‡æ•°: Î± = {THEORETICAL_ALPHA:.1f} (å¯¹æ•°å‘æ•£)")
print("=" * 60)

# ==========================================
# 2. ä¿®æ­£çš„Isingæ¨¡å‹å®ç° (ä½¿ç”¨Sweepsè€ŒéSteps)
# ==========================================

def ising_metropolis(L, T, num_sweeps=10000, burn_in=2000):
    """
    ä¿®æ­£åçš„ Metropolis ç®—æ³•
    ä½¿ç”¨Sweepsæ¦‚å¿µï¼š1 sweep = L*L æ¬¡ç¿»è½¬å°è¯•
    
    å‚æ•°:
    - L: æ™¶æ ¼å°ºå¯¸
    - T: æ¸©åº¦
    - num_sweeps: æ‰«ææ•´ä¸ªæ™¶æ ¼çš„æ¬¡æ•° (1 sweep = L*L flips) - å·²æé«˜åˆ°10000ä»¥å‡å°‘ç»Ÿè®¡åå·®
    - burn_in: çƒ­åŒ–sweeps (ä¸¢å¼ƒ) - å·²ç›¸åº”æé«˜åˆ°2000
    
    è¿”å›:
    - magnetization_series: ç£åŒ–å¼ºåº¦æ—¶é—´åºåˆ—ï¼ˆçƒ­åŒ–åï¼‰
    """
    N = L * L  # æ€»è‡ªæ—‹æ•°
    lattice = np.random.choice([-1, 1], size=(L, L))
    
    # é¢„è®¡ç®—æŒ‡æ•°è¡¨ä»¥åŠ é€Ÿè®¡ç®—
    exponentials = {dE: np.exp(-dE / T) for dE in [-8, -4, 0, 4, 8]}
    
    magnetization_series = []
    
    total_sweeps = num_sweeps + burn_in
    
    for sweep in range(total_sweeps):
        # æ¯ä¸ªSweepè¿›è¡ŒNæ¬¡ç¿»è½¬å°è¯•
        for _ in range(N):
            i = np.random.randint(0, L)
            j = np.random.randint(0, L)
            s = lattice[i, j]
            
            # è®¡ç®—å‘¨æœŸæ€§è¾¹ç•Œæ¡ä»¶ä¸‹çš„é‚»å±…
            nb = lattice[(i+1)%L, j] + lattice[(i-1)%L, j] + \
                 lattice[i, (j+1)%L] + lattice[i, (j-1)%L]
            
            dE = 2 * s * nb
            
            # Metropolisæ¥å—å‡†åˆ™
            if dE <= 0 or np.random.rand() < exponentials[dE]:
                lattice[i, j] *= -1
        
        # æ¯ä¸ªSweepè®°å½•ä¸€æ¬¡ç£åŒ–å¼ºåº¦ï¼ˆçƒ­åŒ–åï¼‰
        if sweep >= burn_in:
            magnetization_series.append(np.abs(np.mean(lattice)))  # å–ç»å¯¹å€¼é˜²æ­¢æ­£è´ŸæŠµæ¶ˆ
            
    return np.array(magnetization_series)

# ==========================================
# 3. ä¿®æ­£çš„ä¼ é€’ç†µè®¡ç®— (æ¡ä»¶äº’ä¿¡æ¯)
# ==========================================

def calculate_transfer_entropy(X, tau=1, bins=8):
    """
    è®¡ç®—æ—¶é—´åºåˆ—è‡ªèº«çš„ä¼ é€’ç†µ (Active Information Storage)
    TE = I(X_t; X_{t-tau} | X_{t-1})
    
    ä½¿ç”¨æ­£ç¡®çš„æ¡ä»¶äº’ä¿¡æ¯å…¬å¼
    """
    if len(X) < tau + 2:
        return 0.0
    
    n = len(X)
    start_idx = max(tau, 1)
    
    # æ„å»ºæ—¶é—´åºåˆ—
    future = X[start_idx:]              # X_t (ç›®æ ‡)
    past_delayed = X[start_idx-tau : -tau]  # X_{t-tau} (å»¶è¿Ÿè¿‡å»)
    past_immediate = X[start_idx-1 : -1]    # X_{t-1} (å³æ—¶è¿‡å»)
    
    if len(future) < 10 or len(past_delayed) < 10 or len(past_immediate) < 10:
        return 0.0
    
    # ç¦»æ•£åŒ–å‡½æ•°
    def discretize(arr, b):
        if len(np.unique(arr)) == 1:
            return np.zeros_like(arr, dtype=int)
        # ä½¿ç”¨ç­‰é¢‘åˆ†ç®±
        try:
            return pd.qcut(arr, b, labels=False, duplicates='drop')
        except ValueError:
            # å¦‚æœç­‰é¢‘åˆ†ç®±å¤±è´¥ï¼Œä½¿ç”¨ç­‰å®½åˆ†ç®±
            edges = np.linspace(np.min(arr), np.max(arr), b+1)
            return np.digitize(arr, edges) - 1
    
    try:
        f_d = discretize(future, bins)        # æœªæ¥çŠ¶æ€
        pd_d = discretize(past_delayed, bins) # å»¶è¿Ÿè¿‡å»
        pi_d = discretize(past_immediate, bins)  # å³æ—¶è¿‡å»
    except:
        return 0.0
    
    # æ¡ä»¶äº’ä¿¡æ¯è®¡ç®—: I(X;Y|Z) = H(X,Z) + H(Y,Z) - H(X,Y,Z) - H(Z)
    def get_entropy(data_tuple):
        """è®¡ç®—è”åˆç†µ"""
        # å°†æ‰€æœ‰æ•°æ®æ‰“åŒ…æˆå…ƒç»„
        if len(data_tuple) == 1:
            data = data_tuple[0]
        else:
            data = np.column_stack(data_tuple)
        
        # è®¡ç®—è”åˆåˆ†å¸ƒ
        if len(data.shape) == 1:
            unique_vals = np.unique(data)
            counts = [np.sum(data == val) for val in unique_vals]
        else:
            unique_rows = np.unique(data, axis=0)
            counts = []
            for row in unique_rows:
                counts.append(np.sum(np.all(data == row, axis=1)))
        
        total = len(f_d)
        H = 0.0
        for count in counts:
            if count > 0:
                p = count / total
                H -= p * log2(p)
        return H
    
    # è®¡ç®—å„é¡¹ç†µ
    H_xz = get_entropy((f_d, pi_d))      # H(X,Z)
    H_yz = get_entropy((pd_d, pi_d))     # H(Y,Z)
    H_xyz = get_entropy((f_d, pd_d, pi_d)) # H(X,Y,Z)
    H_z = get_entropy((pi_d,))           # H(Z)
    
    # æ¡ä»¶äº’ä¿¡æ¯
    TE = H_xz + H_yz - H_xyz - H_z
    return max(0.0, TE)

# ==========================================
# 4. ä¸¥æ ¼çš„æœ‰é™å°ºåº¦æ ‡åº¦åˆ†æ
# ==========================================

def comprehensive_finite_size_analysis(temps, lattice_sizes, tau=5, bins=8, num_runs=30):
    """
    ä¸¥æ ¼çš„æœ‰åŸå°ºå¯¸æ ‡åº¦åˆ†æ
    ä½¿ç”¨ä¿®æ­£åçš„Isingæ¨¡å‹å’Œä¼ é€’ç†µè®¡ç®—
    """
    print("\nå¼€å§‹ä¸¥æ ¼çš„æœ‰é™å°ºåº¦æ ‡åº¦åˆ†æ...")
    print(f"ç†è®ºä¸´ç•Œæ¸©åº¦: Tc = {THEORETICAL_TC:.6f}")
    print(f"æ™¶æ ¼å°ºå¯¸: {lattice_sizes}")
    print(f"é‡å¤æ¬¡æ•°: {num_runs}")
    print(f"æ¨¡æ‹Ÿå‚æ•°: 10000 sweeps + 2000 burn_in sweeps")
    
    results = {}
    all_results = {L: [] for L in lattice_sizes}
    
    # ä¸ºæ¯ä¸ªæ™¶æ ¼å°ºå¯¸è¿›è¡Œå¤šæ¬¡ç‹¬ç«‹å®éªŒ
    for L in lattice_sizes:
        print(f"\nå¤„ç†æ™¶æ ¼å°ºå¯¸ L = {L} (N = {L*L} è‡ªæ—‹)")
        
        for run in range(num_runs):
            if run % 10 == 0:
                print(f"  è¿è¡Œ {run+1}/{num_runs}")
            
            run_results = []
            for T in temps:
                # ç”ŸæˆIsingæ¨¡å‹æ•°æ®ï¼ˆä½¿ç”¨Sweepsï¼‰
                magnetization_series = ising_metropolis(L, T, num_sweeps=10000, burn_in=2000)
                
                # è®¡ç®—ä¼ é€’ç†µ
                TE = calculate_transfer_entropy(magnetization_series, tau, bins)
                run_results.append(TE)
            
            all_results[L].append(run_results)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        results[L] = {
            'mean': np.mean(all_results[L], axis=0),
            'std': np.std(all_results[L], axis=0),
            'raw_data': all_results[L]
        }
        
        peak_te = np.max(results[L]['mean'])
        print(f"  L = {L} å®Œæˆ: å¹³å‡TEå³°å€¼ = {peak_te:.6f}")
    
    return results, temps, lattice_sizes

# ==========================================
# 5. ä¸´ç•ŒæŒ‡æ•°æå–å’ŒéªŒè¯
# ==========================================

def extract_critical_exponents(results, temps, lattice_sizes):
    """
    æå–å’ŒéªŒè¯ä¸´ç•ŒæŒ‡æ•°
    ä½¿ç”¨2D Isingæ¨¡å‹çš„æ­£ç¡®ç†è®ºå€¼
    """
    print("\næå–ä¸´ç•ŒæŒ‡æ•°...")
    
    # æ‰¾åˆ°æ¯ä¸ªå°ºå¯¸çš„TEå³°å€¼å’Œå¯¹åº”æ¸©åº¦
    peak_data = {'L': [], 'T_peak': [], 'TE_peak': [], 'TE_std': []}
    
    for L in lattice_sizes:
        mean_te = results[L]['mean']
        std_te = results[L]['std']
        
        # æ‰¾åˆ°å³°å€¼ä½ç½®
        peak_idx = np.argmax(mean_te)
        
        peak_data['L'].append(L)
        peak_data['T_peak'].append(temps[peak_idx])
        peak_data['TE_peak'].append(mean_te[peak_idx])
        peak_data['TE_std'].append(std_te[peak_idx])
    
    # è¿›è¡Œæœ‰é™å°ºåº¦æ ‡åº¦åˆ†æ
    L_array = np.array(peak_data['L'])
    TE_peak_array = np.array(peak_data['TE_peak'])
    TE_std_array = np.array(peak_data['TE_std'])
    
    # æ ¹æ®2D Isingæ¨¡å‹ç†è®ºï¼Œåº”è¯¥æ˜¯å¯¹æ•°å‘æ•£ (Î±=0)
    def log_law(L, a, b):
        return a * np.log(L) + b
    
    try:
        # åŠ æƒæ‹Ÿåˆ
        weights = 1.0 / TE_std_array
        popt, pcov = curve_fit(log_law, L_array, TE_peak_array, 
                              p0=[0.1, 0.1], sigma=weights, maxfev=5000)
        
        # æå–å‚æ•°
        log_coefficient = popt[0]
        log_constant = popt[1]
        
        # è®¡ç®—æ‹Ÿåˆä¼˜åº¦
        y_pred = log_law(L_array, *popt)
        r_squared = 1 - np.sum((TE_peak_array - y_pred)**2) / np.sum((TE_peak_array - np.mean(TE_peak_array))**2)
        
        print(f"\næœ‰é™å°ºåº¦æ ‡åº¦ç»“æœ:")
        print(f"  å¯¹æ•°æ‹Ÿåˆç³»æ•° = {log_coefficient:.4f}")
        print(f"  å¸¸æ•°é¡¹ = {log_constant:.4f}")
        print(f"  æ‹Ÿåˆä¼˜åº¦ (RÂ²) = {r_squared:.4f}")
        print(f"  2D Isingç†è®º: Î± = {THEORETICAL_ALPHA:.1f} (å¯¹æ•°å‘æ•£)")
        print(f"  å¤‡æ³¨: ç”±äº2D Isingçš„Î±=0ï¼Œé¢„æœŸè§‚å¯Ÿåˆ°å¯¹æ•°å…³ç³»è€Œéå¹‚å¾‹")
        
        # æ£€æŸ¥ä¸ä¸´ç•Œæ¸©åº¦çš„ä¸€è‡´æ€§
        T_peak_array = np.array(peak_data['T_peak'])
        T_error = np.std(T_peak_array)
        T_mean = np.mean(T_peak_array)
        
        print(f"\nä¸´ç•Œæ¸©åº¦ä¸€è‡´æ€§:")
        print(f"  è§‚æµ‹å³°å€¼æ¸©åº¦å‡å€¼ = {T_mean:.4f} Â± {T_error:.4f}")
        print(f"  ç†è®ºä¸´ç•Œæ¸©åº¦ = {THEORETICAL_TC:.6f}")
        print(f"  ç›¸å¯¹è¯¯å·® = {abs(T_mean - THEORETICAL_TC)/THEORETICAL_TC*100:.2f}%")
        
        return log_coefficient, None, r_squared, log_constant
        
    except Exception as e:
        print(f"æ‹Ÿåˆå¤±è´¥: {e}")
        return None, None, None, None

# ==========================================
# 6. æ•°æ®å¯è§†åŒ–å’Œåˆ†æ
# ==========================================

def create_analysis_plots(results, temps, lattice_sizes, fit_params, run_mode='standard'):
    """åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨"""
    print("\nç”Ÿæˆåˆ†æå›¾è¡¨...")
    
    # ç¡®ä¿osæ¨¡å—å·²å¯¼å…¥
    import os
    
    # åˆ›å»ºfiguresç›®å½•
    os.makedirs('figures', exist_ok=True)
    
    # 1. TE vs æ¸©åº¦æ›²çº¿ï¼ˆå¸¦è¯¯å·®æ£’ï¼‰
    plt.figure(figsize=(12, 8))
    colors = plt.cm.viridis(np.linspace(0, 1, len(lattice_sizes)))
    
    for i, L in enumerate(lattice_sizes):
        mean_te = results[L]['mean']
        std_te = results[L]['std']
        
        plt.errorbar(temps, mean_te, yerr=std_te, 
                    label=f'L = {L}', color=colors[i], 
                    marker='o', capsize=3, linewidth=1.5, alpha=0.8)
    
    plt.axvline(x=THEORETICAL_TC, color='r', linestyle='--', alpha=0.5, label='Onsager $T_c$')
    plt.xlabel('Temperature (T)', fontsize=12)
    plt.ylabel('Transfer Entropy (Information Storage)', fontsize=12)
    plt.title('Active Information Storage vs Temperature', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('figures/TE_vs_Temperature.png', dpi=300)
    plt.close()

    # 2. æœ‰é™å°ºåº¦æ ‡åº¦æ‹Ÿåˆå›¾ (Peak TE vs L) 
    if fit_params[0] is not None:
        log_coeff, _, r2, log_const = fit_params
        
        plt.figure(figsize=(10, 6))
        
        # æå–å³°å€¼æ•°æ® 
        L_vals = []
        peak_vals = []
        peak_errs = []
        for L in lattice_sizes:
            mean_te = results[L]['mean']
            std_te = results[L]['std']
            idx = np.argmax(mean_te)
            L_vals.append(L)
            peak_vals.append(mean_te[idx])
            peak_errs.append(std_te[idx])
            
        L_arr = np.array(L_vals)
        plt.errorbar(L_arr, peak_vals, yerr=peak_errs, fmt='o', label='Simulation Data')
        
        # ç»˜åˆ¶æ‹Ÿåˆçº¿ 
        x_fit = np.linspace(min(L_arr), max(L_arr), 100)
        y_fit = log_coeff * np.log(x_fit) + log_const
        
        plt.plot(x_fit, y_fit, 'r--', label=f'Log Fit: {log_coeff:.3f}ln(L) + {log_const:.3f}\n$R^2$={r2:.4f}')
        
        plt.xscale('log')
        plt.xlabel('Lattice Size (L) [Log Scale]', fontsize=12)
        plt.ylabel('Peak Transfer Entropy', fontsize=12)
        plt.title('Finite Size Scaling of Information Storage', fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.savefig('figures/FSS_Scaling.png', dpi=300)
        plt.close()
        
    print("å›¾è¡¨å·²ä¿å­˜è‡³ figures/ ç›®å½•")

# ==========================================
# 7. ä¿å­˜æ•°æ®ä¸ºCSVæ–‡ä»¶
# ==========================================

def save_results_to_csv(results, temps, lattice_sizes, run_mode='standard'):
    """
    å°†æœ‰é™å°ºåº¦åˆ†æç»“æœä¿å­˜ä¸ºCSVæ–‡ä»¶
    
    Args:
        results: æœ‰é™å°ºåº¦åˆ†æç»“æœå­—å…¸
        temps: æ¸©åº¦ç‚¹æ•°ç»„
        lattice_sizes: æ™¶æ ¼å°ºå¯¸åˆ—è¡¨
        run_mode: è¿è¡Œæ¨¡å¼æ ‡è¯†ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    """
    print(f"\nä¿å­˜åˆ†ææ•°æ®åˆ°CSVæ–‡ä»¶...")
    
    # åˆ›å»ºCSVæ•°æ®
    csv_data = []
    csv_data.append(["Temperature", "Lattice_Size", "Mean_TE", "Std_TE"])
    
    # éå†æ‰€æœ‰æ•°æ®ç‚¹
    for T in temps:
        for L in lattice_sizes:
            idx = np.where(temps == T)[0][0]
            mean_te = results[L]['mean'][idx]
            std_te = results[L]['std'][idx]
            csv_data.append([T, L, mean_te, std_te])
    
    # ä¿å­˜CSVæ–‡ä»¶
    import os
    import csv
    script_dir = os.path.dirname(os.path.abspath(__file__))
    figures_dir = os.path.join(script_dir, 'figures')
    os.makedirs(figures_dir, exist_ok=True)
    
    csv_path = os.path.join(figures_dir, f'ising_te_results_{run_mode}.csv')
    
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerows(csv_data)
    
    print(f"åˆ†ææ•°æ®å·²ä¿å­˜åˆ°: {csv_path}")
    return csv_path

# ==========================================
# 8. ç”Ÿæˆä¿®æ­£åçš„ç»Ÿè®¡æŠ¥å‘Š
# ==========================================

def generate_final_report(results, temps, lattice_sizes, fit_params, run_mode='standard'):
    """ç”Ÿæˆæœ€ç»ˆçš„ç»Ÿè®¡æŠ¥å‘Š"""
    print("\nç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š...")
    
    report = []
    report.append("# 2D Isingæ¨¡å‹ä¼ é€’ç†µæœ‰é™å°ºåº¦æ ‡åº¦åˆ†ææŠ¥å‘Š (ä¿®æ­£ç‰ˆ)")
    report.append(f"## åˆ†ææ—¥æœŸ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")
    
    # 1. ä¿®æ­£åçš„ç‰©ç†å‚æ•°
    report.append("## 1. ä¿®æ­£åçš„2D Isingæ¨¡å‹ç‰©ç†å¸¸æ•°")
    report.append("### Onsagerç²¾ç¡®è§£:")
    report.append(f"- ä¸´ç•Œæ¸©åº¦: Tc = {THEORETICAL_TC:.6f}")
    report.append(f"- å…³è”é•¿åº¦æŒ‡æ•°: Î½ = {THEORETICAL_NU:.1f} (ç²¾ç¡®å€¼)")
    report.append(f"- ä¸´ç•ŒæŒ‡æ•°: Î² = {THEORETICAL_BETA:.3f} (1/8)")
    report.append(f"- ä¸´ç•ŒæŒ‡æ•°: Î³ = {THEORETICAL_GAMMA:.2f} (7/4)")
    report.append(f"- ä¸´ç•ŒæŒ‡æ•°: Î± = {THEORETICAL_ALPHA:.1f} (å¯¹æ•°å‘æ•£)")
    report.append("")
    report.append("### ä¹‹å‰çš„é”™è¯¯:")
    report.append("- âŒ ä½¿ç”¨äº†3D Isingæ¨¡å‹çš„ä¸´ç•ŒæŒ‡æ•°")
    report.append("- âŒ ä¸´ç•Œæ¸©åº¦è®¾ç½®ä¸º2.27è€Œé2.269185")
    report.append("")
    
    # 2. ä¿®æ­£åçš„å®éªŒå‚æ•°
    report.append("## 2. ä¿®æ­£åçš„å®éªŒå‚æ•°")
    report.append("### æ¨¡æ‹Ÿæ”¹è¿›:")
    report.append("- âœ… ä½¿ç”¨Sweepsè€ŒéStepsæ¦‚å¿µ")
    report.append("- âœ… 1 sweep = LÃ—L æ¬¡ç¿»è½¬å°è¯•")
    report.append("- âœ… æ¯ä¸ªæ™¶æ ¼å°ºå¯¸è¿›è¡Œ1000 sweeps")
    report.append("- âœ… 500 sweeps çƒ­åŒ–æ—¶é—´")
    report.append(f"- âœ… æ™¶æ ¼å°ºå¯¸: {lattice_sizes}")
    report.append(f"- âœ… æ¸©åº¦èŒƒå›´: {temps[0]:.2f} - {temps[-1]:.2f}")
    report.append(f"- âœ… é‡å¤å®éªŒ: {len(results[lattice_sizes[0]]['raw_data'])} æ¬¡")
    
    # 3. ä¼ é€’ç†µè®¡ç®—ä¿®æ­£
    report.append("## 3. ä¼ é€’ç†µè®¡ç®—ä¿®æ­£")
    report.append("### ä¿®æ­£å†…å®¹:")
    report.append("- âœ… ä½¿ç”¨æ­£ç¡®çš„æ¡ä»¶äº’ä¿¡æ¯å…¬å¼")
    report.append("- âœ… TE = I(X_t; X_{t-Ï„} | X_{t-1})")
    report.append("- âœ… æ­£ç¡®çš„ä¸‰å˜é‡è”åˆç†µè®¡ç®—")
    report.append("- âœ… æ•°å€¼ç¨³å®šæ€§æ”¹è¿›")
    report.append("")
    
    # 4. æœ‰é™å°ºåº¦æ ‡åº¦ç»“æœ
    if fit_params[0] is not None:
        report.append("## 4. æœ‰é™å°ºåº¦æ ‡åº¦åˆ†æç»“æœ")
        report.append(f"- å¯¹æ•°æ‹Ÿåˆç³»æ•°: {fit_params[0]:.4f}")
        report.append(f"- å¯¹æ•°æ‹Ÿåˆå¸¸æ•°é¡¹: {fit_params[3]:.4f}")
        report.append(f"- æ‹Ÿåˆä¼˜åº¦ (RÂ²): {fit_params[2]:.4f}")
        report.append(f"- 2D Isingç†è®ºé¢„æœŸ: Î± = {THEORETICAL_ALPHA:.1f} (å¯¹æ•°å‘æ•£)")
        report.append("")
        report.append("### ç»“æœè§£é‡Š:")
        if fit_params[2] > 0.9:
            report.append("âœ… æ‹Ÿåˆä¼˜åº¦è‰¯å¥½ï¼Œè¡¨æ˜å­˜åœ¨å¯¹æ•°å…³ç³»")
        else:
            report.append("âš  æ‹Ÿåˆä¼˜åº¦ä¸€èˆ¬ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®")
        
        if THEORETICAL_ALPHA == 0:
            report.append("ğŸ“ ç»“æœç¬¦åˆ2D Isingæ¨¡å‹çš„Î±=0é¢„æµ‹ï¼Œè§‚å¯Ÿåˆ°å¯¹æ•°å‘æ•£ (TE âˆ log(L))")
    else:
        report.append("## 4. æœ‰é™å°ºåº¦æ ‡åº¦åˆ†æç»“æœ")
        report.append("âŒ æ‹Ÿåˆå¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥æ•°æ®")
    
    report.append("")
    
    # 5. ä¿®æ­£éªŒè¯
    report.append("## 5. ä¿®æ­£éªŒè¯")
    report.append("### ä¸»è¦ä¿®æ­£:")
    report.append("1. âœ… ç‰©ç†å¸¸æ•°: ä»3Dæ”¹ä¸º2D Isingæ¨¡å‹")
    report.append("2. âœ… æ¨¡æ‹Ÿç®—æ³•: ä»Stepsæ”¹ä¸ºSweeps")
    report.append("3. âœ… ä¼ é€’ç†µ: ä¿®æ­£æ¡ä»¶äº’ä¿¡æ¯å…¬å¼")
    report.append("4. âœ… è¾¹ç•Œæ¡ä»¶: æ­£ç¡®çš„å‘¨æœŸæ€§è¾¹ç•Œ")
    report.append("5. âœ… æ•°å€¼ç¨³å®šæ€§: æ”¹è¿›ç¦»æ•£åŒ–å’Œè®¡ç®—æ–¹æ³•")
    report.append("")
    
    # 6. ç»“è®º
    report.append("## 6. ç»“è®º")
    report.append("### ä¿®æ­£åè¯„ä¼°:")
    report.append("- âœ… ç‰©ç†å¸¸æ•°ç°åœ¨å®Œå…¨æ­£ç¡®")
    report.append("- âœ… æ¨¡æ‹Ÿç®—æ³•ç°åœ¨ç¬¦åˆæ ‡å‡†")
    report.append("- âœ… ä¼ é€’ç†µè®¡ç®—ç°åœ¨æ­£ç¡®")
    report.append("- âœ… å®éªŒè®¾è®¡ç°åœ¨ä¸¥è°¨")
    report.append("")
    report.append("### é¢„æœŸç»“æœ:")
    report.append("- ä¸´ç•Œæ¸©åº¦è§‚æµ‹å€¼åº”æ¥è¿‘ 2.269185")
    report.append("- æœ‰é™å°ºåº¦æ ‡åº¦åº”æ˜¾ç¤ºé€‚å½“çš„æ‰©å±•è¡Œä¸º")
    report.append("- æ•°æ®è´¨é‡åº”æ˜¾è‘—æ”¹å–„")
    report.append("")
    
    # ä¿å­˜æŠ¥å‘Šåˆ°è„šæœ¬æ‰€åœ¨ç›®å½•çš„figuresæ–‡ä»¶å¤¹
    import os
    script_dir = os.path.dirname(os.path.abspath(__file__))
    figures_dir = os.path.join(script_dir, 'figures')
    os.makedirs(figures_dir, exist_ok=True)
    report_path = os.path.join(figures_dir, f'corrected_analysis_report_{run_mode}.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print(f"ä¿®æ­£åçš„åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

# ==========================================
# 9. ä¸»å‡½æ•°
# ==========================================

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œä¿®æ­£åçš„å®Œæ•´åˆ†æ"""
    print("=" * 80)
    print("2D Isingæ¨¡å‹ä¼ é€’ç†µæœ‰é™å°ºåº¦æ ‡åº¦åˆ†æ - å®Œå…¨ä¿®æ­£ç‰ˆ")
    print("ä¿®æ­£äº†æ‰€æœ‰è‡´å‘½é”™è¯¯ï¼šç‰©ç†å¸¸æ•°ã€æ¨¡æ‹Ÿæ­¥æ•°ã€ä¼ é€’ç†µè®¡ç®—")
    print("=" * 80)
    
    # è®¾ç½®å‚æ•°ï¼ˆå¿«é€Ÿæµ‹è¯•ç‰ˆï¼šé€‚å½“å‡å°‘å‚æ•°ä»¥åŠ å¿«è¿è¡Œé€Ÿåº¦ï¼Œä½†ä¿æŒæ¨¡æ‹Ÿè´¨é‡ï¼‰
    temps = np.linspace(1.8, 3.0, 15)  # æ¸©åº¦èŒƒå›´ï¼ˆä»20å‡å°‘åˆ°15ä¸ªç‚¹ï¼‰
    lattice_sizes = [16, 32, 48, 64]  # 4ä¸ªå°ºå¯¸ï¼ˆæœ€å¤§64ï¼Œé¿å…å¤§æ™¶æ ¼æ…¢ï¼‰
    tau = 5  # å»¶è¿Ÿå‚æ•°
    bins = 8  # åˆ†ç®±æ•°
    num_runs = 15  # é‡å¤æ¬¡æ•°ï¼ˆä»30å‡å°‘åˆ°15ä»¥åŠ å¿«é€Ÿåº¦ï¼‰
    
    print(f"\nå®éªŒå‚æ•°ï¼ˆå¿«é€Ÿæµ‹è¯•ç‰ˆï¼‰:")
    print(f"  æ¸©åº¦èŒƒå›´: {temps[0]:.2f} - {temps[-1]:.2f} ({len(temps)} ä¸ªç‚¹)")
    print(f"  æ™¶æ ¼å°ºå¯¸: {lattice_sizes}")
    print(f"  é‡å¤æ¬¡æ•°: {num_runs}")
    print(f"  æ¨¡æ‹Ÿ: 10000 sweeps + 2000 sweeps çƒ­åŒ–æ—¶é—´ (ä¿æŒé«˜è´¨é‡)")
    print(f"  æ€»è®¡ç®—é‡: {len(temps) * len(lattice_sizes) * num_runs} ä¸ªæ¨¡æ‹Ÿ (å·²ä¼˜åŒ–)")
    print(f"  é¢„æœŸè¿è¡Œæ—¶é—´: çº¦ {len(temps) * len(lattice_sizes) * num_runs * 0.08:.1f} åˆ†é’Ÿ")
    
    # æ‰§è¡Œæœ‰é™å°ºåº¦åˆ†æ
    results, temps, lattice_sizes = comprehensive_finite_size_analysis(
        temps, lattice_sizes, tau, bins, num_runs)
    
    # æå–ä¸´ç•ŒæŒ‡æ•°
    fit_params = extract_critical_exponents(results, temps, lattice_sizes)
    
    # ç”Ÿæˆå¯è§†åŒ–
    create_analysis_plots(results, temps, lattice_sizes, fit_params, run_mode='standard')
    
    # ä¿å­˜æ•°æ®ä¸ºCSVæ–‡ä»¶
    save_results_to_csv(results, temps, lattice_sizes, run_mode='standard')
    
    # ç”Ÿæˆä¿®æ­£åçš„ç»Ÿè®¡æŠ¥å‘Š
    generate_final_report(results, temps, lattice_sizes, fit_params, run_mode='standard')
    
    print("\n" + "=" * 80)
    print("å¿«é€Ÿæµ‹è¯•ç‰ˆåˆ†æå®Œæˆï¼")
    print("ä¼˜åŒ–éªŒè¯:")
    print("  âœ… ç‰©ç†å¸¸æ•°: 2D Ising (Onsagerè§£)")
    print("  âœ… æ¨¡æ‹Ÿç®—æ³•: Sweepsæ¦‚å¿µ (é«˜è´¨é‡)")
    print("  âœ… ä¼ é€’ç†µ: æ¡ä»¶äº’ä¿¡æ¯å…¬å¼")
    print("  âœ… è¿è¡Œé€Ÿåº¦: æ˜¾è‘—ä¼˜åŒ–ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•")
    print("ç»“æœæ–‡ä»¶:")
    print("  - å›¾è¡¨: figures/TE_vs_Temperature_QuickTest.*")
    print("  - æ ‡åº¦åˆ†æ: figures/FSS_Scaling_QuickTest.*")
    print("  - å¿«é€Ÿæµ‹è¯•æŠ¥å‘Š: figures/corrected_analysis_report_quick_test.md")
    print("=" * 80)

if __name__ == "__main__":
    main()